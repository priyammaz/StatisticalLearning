{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd95b5e",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "#### Name: Priyam Mazumdar\n",
    "#### NetID: priyamm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de161440",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Packages ###\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "287f2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "np.random.seed(542)\n",
    "n = 150\n",
    "x = np.random.uniform(low=0, high=1, size=(n, 1))\n",
    "X = np.hstack([np.ones(shape=(150,1)), x])\n",
    "args = np.array([0.5, 1])\n",
    "y = X.dot(args) + np.random.standard_normal(150)\n",
    "y[x.argmin()] = -30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561dd2f",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "a) [5 pts] Fit an OLS model with the regular ℓ2\n",
    " loss. Report your coefficients (do not report other information). Although this is only one set of samples, but do you expect this estimator to be biased based on how we set up the observed data? Do you expect the parameter β1\n",
    " to bias upwards or downwards? Explain your reason. Hint: is the outlier pulling the regression line slope up or down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66468029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Parameters B0: -0.32252335404905885 B1: 2.199452488577025\n"
     ]
    }
   ],
   "source": [
    "def OLS(X, y):\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    " \n",
    "coefficients = OLS(X, y)\n",
    "print(\"OLS Parameters B0:\", coefficients[0], \"B1:\", coefficients[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802295d",
   "metadata": {},
   "source": [
    "The presence of this outlier will cause a major bias in our parameters and the regression model will poorly predict the training data. We can expect the outlier to bias our $\\beta_1$ upwards. The reason for this is we set our minimum X as -30 which will force our leftside of the regression line downwards to minimize OLS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59152ec",
   "metadata": {},
   "source": [
    "\n",
    "b) [10 pts] Define your own Huber loss function huberLoss(b, trainX, trainY) given a set of observed data with tuning parameter δ=1\n",
    ". Here, b is a p\n",
    "-dim parameter vector, trainX is a n×p\n",
    " design matrix and trainY\n",
    " is the outcome. This function should return a scalar as the empirical loss. You can use our Huber function in your own code. After defining this loss function, use the optim() function to solve the parameter estimates. Finally, report your coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11edab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Parameters B0: 0.42098531186094384 B1: 1.0660410224987118\n",
      "Huber Loss from Optimize Parameters 0.5481708305815473\n"
     ]
    }
   ],
   "source": [
    "class HuberLoss:\n",
    "    def __init__(self, trainX, trainY):\n",
    "        self.train_X = trainX\n",
    "        self.train_y = trainY\n",
    "        self.b_guess = [0,0]\n",
    "        self.delta = 1\n",
    "    \n",
    "    def loss(self, b):\n",
    "        b = np.array(b)\n",
    "        diff = self.train_y - self.train_X.dot(b)\n",
    "        # MSE Loss if less than delta\n",
    "        diff[np.where(abs(diff)<=self.delta)] = 0.5*diff[np.where(abs(diff)<=self.delta)]**2\n",
    "        # ABS Loss if greater than delta\n",
    "        diff[np.where(abs(diff)>self.delta)] = self.delta*np.abs(diff[np.where(abs(diff)>self.delta)]) - self.delta/2\n",
    "\n",
    "        return np.mean(diff)\n",
    "    \n",
    "    def optimize(self):\n",
    "        result = optimize.minimize(self.loss, self.b_guess, method=\"BFGS\")\n",
    "        return result.x\n",
    "\n",
    "HL = HuberLoss(X, y)\n",
    "optimized_parameters = HL.optimize()\n",
    "print(\"Optimizer Parameters B0:\", optimized_parameters[0], \"B1:\", optimized_parameters[1])\n",
    "print(\"Huber Loss from Optimize Parameters\", HL.loss(optimized_parameters))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21788bd5",
   "metadata": {},
   "source": [
    "c) [20 pts] We still do not know which method performs better in this case. Let’s use a simulation study to compare the two methods. Complete the following\n",
    "\n",
    "Set up a simulation for 1000 times. At each time, randomly generate a set of observed data, but also force the outlier with our code y[which.min(X[, 2])] = -30.\n",
    "Fit the regression model with ℓ2 loss and Huber loss, and record the slope variable estimates.\n",
    "Make a side-by-side boxplot to show how these two methods differ in terms of the estimations. Which method seem to have more bias? and report the amount of bias based on your simulation. What can you conclude from the results? Does this match your expectation in part a)? Can you explain this (both OLS and Huber) with the form of loss function, in terms of what their effects are?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd307055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    n = 150\n",
    "    x = np.random.uniform(low=0, high=1, size=(n, 1))\n",
    "    X = np.hstack([np.ones(shape=(150,1)), x])\n",
    "    args = np.array([0.5, 1])\n",
    "    y = X.dot(args) + np.random.standard_normal(150)\n",
    "    y[x.argmin()] = -30\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "ols = []\n",
    "huber = []\n",
    "for i in range(1000):\n",
    "    X, y = gen_data()\n",
    "    ### OLS Estimate ###\n",
    "    ols_coeff = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    ols.append(list(ols_coeff))\n",
    "    \n",
    "    ### Huber Loss ###\n",
    "    HL = HuberLoss(X, y)\n",
    "    optimized_parameters = HL.optimize()\n",
    "    huber.append(list(optimized_parameters))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eb05799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias of B1 from Huber: 0.07698027531137908\n",
      "Bias of B1 from OLS: 1.2381307649427304\n"
     ]
    }
   ],
   "source": [
    "### Calculate Bias\n",
    "avg_b1_huber = np.array(huber).mean(axis=0)[1]\n",
    "avg_b1_ols = np.array(ols).mean(axis=0)[1]\n",
    "print(\"Bias of B1 from Huber: {}\".format(avg_b1_huber - 1))\n",
    "print(\"Bias of B1 from OLS: {}\".format(avg_b1_ols - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b85d3638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coeff_estimates = pd.DataFrame(np.hstack([np.array(ols)[:, 1].reshape(1000,1),np.array(huber)[:, 1].reshape(1000,1)]))\n",
    "coeff_estimates.columns = [\"ols_b1\", \"huber_b1\"]\n",
    "coeff_estimates.boxplot()\n",
    "plt.title(\"Coefficients from OLS and Huber Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf36da",
   "metadata": {},
   "source": [
    "We can clearly tell that our OLS Regression had much more bias than huber (Huber has a bias of 0.076 wherease OLS had a bias of 1.23 for our B1 term). We would expect our model intercept to be around 0.5 with a slope of 1, as we indicated in the toy model, and OLS greatly deviated from this, whereas Huber was much closer to our expectation. Therefore we can say that OLS is very sensitive to outliers. This behavior is also expected from our loss functions, as OLS would put a massive penalty on our outlier, making it a point of high leverage on the regression. Huber on the other hand is able to handle our outliers by not putting large penalties on them, therefore giving more reliable coefficients that are representative of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d34a1a",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "a) [10 pts] Fit an OLS estimator with the original data Y_org and X_org by lm(). Also, fit another OLS with scaled data by lm(). Report the coefficients/parameters. Then, transform coefficients from the second approach back to its original scale, and match with the first approach. Summarize your results in a single table: The rows should contain three methods: OLS, OLS Scaled, and OLS Recovered, and there should be four columns that represents the coefficients for each method. You can consider using the kable function, but it is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca9dbb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B0</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unscaled</th>\n",
       "      <td>0.13069</td>\n",
       "      <td>0.73809</td>\n",
       "      <td>2.47960</td>\n",
       "      <td>0.11529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaled</th>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.36418</td>\n",
       "      <td>0.77557</td>\n",
       "      <td>0.04084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recovered</th>\n",
       "      <td>0.13069</td>\n",
       "      <td>0.73809</td>\n",
       "      <td>2.47960</td>\n",
       "      <td>0.11529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                B0       B1       B2       B3\n",
       "Unscaled   0.13069  0.73809  2.47960  0.11529\n",
       "Scaled    -0.00000  0.36418  0.77557  0.04084\n",
       "Recovered  0.13069  0.73809  2.47960  0.11529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Generate Data ###\n",
    "np.random.seed(10)\n",
    "n = 20\n",
    "p = 3\n",
    "\n",
    "V = np.repeat(0.3, 9).reshape(3,3)\n",
    "np.fill_diagonal(V, 1)\n",
    "\n",
    "true_b = np.array([1, 2, 0])\n",
    "X_org = np.random.multivariate_normal(np.repeat(0,3), V, size=n)\n",
    "Y_org = X_org.dot(true_b) + np.random.standard_normal(size=20)\n",
    "\n",
    "X_scaled = ((X_org - X_org.mean(axis=0)) / X_org.std(axis=0))\n",
    "y_scaled  = ((Y_org - Y_org.mean(axis=0))/Y_org.std(axis=0))\n",
    "\n",
    "# Least Squares of UnScaled Data\n",
    "least_squared_unscsaled = LinearRegression().fit(X_org, Y_org)\n",
    "least_squared_coef_unscaled= np.hstack([least_squared_unscsaled.intercept_, least_squared_unscsaled.coef_])\n",
    "# # Least Squares of Scaled Data\n",
    "least_squared_scaled = LinearRegression().fit(X_scaled, y_scaled)\n",
    "least_squared_coef_scaled= np.hstack([least_squared_scaled.intercept_, least_squared_scaled.coef_])\n",
    "\n",
    "# Recover original Coefficients\n",
    "y_org_std = Y_org.std()\n",
    "y_org_mu = Y_org.mean()\n",
    "\n",
    "b0 = y_org_mu - sum([X_org[:, i].mean()*y_org_std*least_squared_coef_scaled[i+1]/X_org[:, i].std() \n",
    "                      for i in range(len(least_squared_coef_scaled)-1)])\n",
    "\n",
    "b1, b2, b3 = [y_org_std*least_squared_coef_scaled[i+1]/X_org[:, i].std() \n",
    "                      for i in range(len(least_squared_coef_scaled)-1)] \n",
    "\n",
    "recovered_coef = [b0, b1, b2, b3]\n",
    "\n",
    "coeff_table = np.array([list(least_squared_coef_unscaled), list(least_squared_coef_scaled), list(recovered_coef)])\n",
    "coeff = pd.DataFrame(coeff_table).round(5)\n",
    "coeff.columns = [\"B0\", \"B1\", \"B2\", \"B3\"]\n",
    "coeff.index = [\"Unscaled\", \"Scaled\", \"Recovered\"]\n",
    "\n",
    "display(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacab7d",
   "metadata": {},
   "source": [
    "As we can see, we were able to fully recover the original coefficients on unscaled data by reconstructing the coefficients from our scaled data.\n",
    "\n",
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36635549",
   "metadata": {},
   "source": [
    "**Coodinate Descent Derivation**\n",
    "\n",
    "$$L(\\beta) = \\sum_{i=1}^n(y_i - \\sum_{j=0}^pX_{ij}\\beta_{j})^2$$\n",
    "\n",
    "$$\\frac{\\partial L(\\beta)}{\\partial \\beta_j} = -2\\sum_{i=1}^n(y_i - \\sum_{j=0}^pX_{ij}\\beta_{j})x_{ij} = 0$$\n",
    "\n",
    "$$-2\\sum_{i=1}^n(y_i - x_{ij}\\beta_j - \\sum_{k\\not=j}^px_{ik}\\beta_{k})x_{ij} = 0$$\n",
    "\n",
    "$$\\beta_j = \\frac{\\sum_{i=1}^nx_{ij}(y_i - \\sum_{k\\not=j}^px_{ik}\\beta_{k})}{\\sum_{i=1}^nx_{ij}^2}$$\n",
    "\n",
    "We can treat the term $(y_i - \\sum_{k\\not=j}^px_{ik}\\beta_{k})$ as removing the effects of all other terms except for our target predictor j\n",
    "\n",
    "\n",
    "### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29a17345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 9 Iterations\n",
      "---Optimized Parameters---\n",
      "B1: 0.3642 B2: 0.7756 B3: 0.0408\n",
      "Loss: 2.795071565502311\n",
      "[3.152824423142539, 2.802065133349259, 2.7952340913220164, 2.795074868192665, 2.7950716188004905, 2.795071566242535, 2.7950715655141827, 2.7950715655025657, 2.795071565502311]\n"
     ]
    }
   ],
   "source": [
    "def coordinate_descent(X, y, iterations=100, tol=1e-7):\n",
    "    n, p = X.shape\n",
    "    curr_beta = [0 for i in range(p)]\n",
    "    beta_list = [0,0,0]\n",
    "    loss_list = []\n",
    "    counter = 0\n",
    "    while True:\n",
    "        for j in range(p):\n",
    "            x_wo_j = np.delete(X, j, axis=1)\n",
    "            b_wo_j = np.delete(np.array(curr_beta), j)\n",
    "            x_j = X[:, j]\n",
    "            curr_beta[j] = (x_j@(y - x_wo_j@b_wo_j)) / (x_j.T@x_j)     \n",
    "            beta_list.append(curr_beta[j])\n",
    "        counter += 1    \n",
    "        loss_list.append(np.sum((y - X@curr_beta)**2))\n",
    "        if (counter >= iterations): \n",
    "            break\n",
    "            \n",
    "        if abs(np.sum(np.array(beta_list)[-6:-3] - np.array(beta_list)[-3:])) < tol:\n",
    "            print(f\"Finished in {counter} Iterations\")\n",
    "            break\n",
    "            \n",
    "\n",
    "             \n",
    "    beta_list = np.array(beta_list).reshape(-1,3)\n",
    "        \n",
    "    print(\"---Optimized Parameters---\")\n",
    "    print(\"B1:\",round(beta_list[-1][0], 4), \"B2:\",round(beta_list[-1][1],4), \"B3:\", round(beta_list[-1][2], 4))\n",
    "    print(\"Loss:\", loss_list[-1])\n",
    "    return beta_list, loss_list\n",
    "        \n",
    "        \n",
    "betas, loss = coordinate_descent(X_scaled, y_scaled) \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079d99f",
   "metadata": {},
   "source": [
    "We can see that our coordinate descent returned the exact values we recived from running OLS on our scaled data previously.\n",
    "\n",
    "### Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f4ea7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_loss = np.sum((y_scaled - least_squared_scaled.predict(X_scaled))**2)\n",
    "convergence = [np.log(loss[i] - actual_loss) for i in range(len(loss))]\n",
    "plt.plot(list(range(1, len(convergence)+1)), convergence)\n",
    "plt.title(\"Log Convergence\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c15e32",
   "metadata": {},
   "source": [
    "We can see that our coordinate descent algorithm has a linear convergence rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
